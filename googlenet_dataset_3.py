# -*- coding: utf-8 -*-
"""GoogLeNet_Dataset_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q-FKqiYAKJgLjekpjhBdxG0LDsP9_eQy
"""

from google.colab import drive
drive.mount("/content/drive", force_remount=True)

import pathlib

dataset_url='/content/drive/MyDrive/plant_leaf_dataset/small_dataset3/'

dataset_dir=pathlib.Path(dataset_url)

dataset_dict={
    'Apple':list(dataset_dir.glob('Apple/*JPG')),
    'Blueberry':list(dataset_dir.glob('Blueberry/*JPG')),
    'Cherry':list(dataset_dir.glob('Cherry/*JPG')),
    'Grapes':list(dataset_dir.glob('Grapes/*JPG'))
}

dataset_label={
    'Apple':0,
    'Blueberry':1,
    'Cherry':2,
    'Grapes':3
}

len(dataset_dict['Cherry'])

import torch
from PIL import Image

# Commented out IPython magic to ensure Python compatibility.
# %%time
# X,y=[],[]
# count=0
# for key in dataset_dict:
#   for values in dataset_dict[key]:
#     image=Image.open(values)
#     X.append(image)
#     y.append(dataset_label[key])
#     count+=1
#     if (count%100==0):
#       print(f"{count} images added to the list")
#     # if(count > 100):
#     #   break
#

"""My modification over Manish's code"""

# import numpy as np

# xNormalized = []

# for image in X:
#   image=np.asarray(image)/255
#   xNormalized.append(image)

from sklearn.model_selection import train_test_split

(X_train,X_test,y_train,y_test)=train_test_split(X,y,test_size=0.2,random_state=0)

print(len(X_train))
print(len(y_train))

import torch
import torch.nn as nn
import torchvision.models as models
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as transforms
from torch.utils.data.dataset import random_split
import numpy as np

# Define global variables:
TOTAL_CLASSES = 4
NUM_EPOCHS = 20
batch_size = 32

# Load googleNet Model
model = models.googlenet()

# GoogLeNet is pre trained with 1000 different classes. Need to change it with the size of our class number.
num_classes = TOTAL_CLASSES #len(classification_class)
in_features = model.fc.in_features
model.fc = nn.Linear(in_features, num_classes)

# Define loss function
criterion = nn.CrossEntropyLoss()

# Optimize
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

#Making the dataset ready for taining out of x_train and y_train

class CustomDataset(Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]

        if self.transform:
            image = self.transform(image)

        return image, label

# Define your transformations
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# Create your Dataset
dataset = CustomDataset(images=X_train, labels=y_train, transform=transform)

image,label=dataset.__getitem__(1)

"""dataset is a CustomDataset object that return tuple of image and label"""

type(dataset)

# Once the dataset is ready split the x_train, y_train dataset into training and validation dataset
# 80% for training 20% for validation
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, validation_dataset = random_split(dataset, [train_size, val_size])
print(len(train_dataset))
print(len(validation_dataset))

# Create data loader to feed the CNN
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)
test_dataset = CustomDataset(images=X_test, labels=y_test, transform=transform)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# for images, labels in train_loader:
#   print(type(images))
#   print(type(labels))
print(len(train_loader))

epochs_list=[]
train_loss_list=[]
validation_loss_list=[]
test_loss_list=[]
training_accuracy_list=[]
validation_accuracy_list=[]
test_accuracy_list=[]

# Iterate over the dataset
for epoch in range(NUM_EPOCHS):
    epochs_list.append(epoch)

    total=0
    correct=0
    batch=0


    accumulated_train_loss=0
    accumulated_validation_loss=0
    accumulated_test_loss=0

    inner_loop_counter_train=0
    inner_loop_counter = 0
    model.train()
    for images, labels in train_loader:
        train_loss=0
        batch+=1
        inner_loop_counter += 1
        # Forward pass
        outputs = model(images)
        # print(labels)
        # print('output........',outputs)
        logits=outputs.logits
        # print('logits.......',logits)
        train_loss = criterion(logits, labels)

        # Backward pass and optimization
        optimizer.zero_grad()
        train_loss.backward()
        optimizer.step()
        _, predicted = torch.max(outputs.logits, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        accumulated_train_loss+=train_loss.detach().numpy().astype(np.float64)
        if (batch%10==0):
          print(f'Loss for {batch}/{len(train_loader)} of {epoch} epochs is: ',train_loss)
        if(batch==2):
          break
    training_accuracy = correct / total
    training_accuracy_list.append(training_accuracy)
    print(f'EPOCH{epoch}.....................')
    print('Accuracy on training set: {}%'.format(100 * training_accuracy))
    train_loss_list.append(accumulated_train_loss/inner_loop_counter)

    correct = 0
    total = 0

    #Evaluation
    # inner_loop_counter_validation=0
    model.eval()
    with torch.inference_mode():
      for images, labels in validation_loader:
        # inner_loop_counter_validation+=1
        validation_loss=0
        outputs = model(images)
        validation_loss=criterion(outputs,labels)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        accumulated_validation_loss+=validation_loss.detach().numpy().astype(np.float64)

      validation_loss_list.append(accumulated_validation_loss/len(validation_loader)) # Code modified
      validation_accuracy=correct/total
      print(f'EPOCH{epoch}.....................')
      print('Accuracy on validation set: {}%'.format(100 * validation_accuracy))
      validation_accuracy_list.append(validation_accuracy)

    correct = 0
    total = 0

    #Testing
    model.eval()
    # inner_loop_counter_test=0
    with torch.inference_mode():
      for images, labels in test_loader:
        # inner_loop_counter_test+=1
        test_loss=0
        outputs = model(images)
        test_loss=criterion(outputs,labels)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        accumulated_test_loss+=test_loss.detach().numpy().astype(np.float64)

      test_loss_list.append(accumulated_test_loss/len(test_loader))
      test_accuracy=correct/total
      print(f'EPOCH{epoch}.....................')
      print('Accuracy on test set: {}%'.format(100 * test_accuracy))
      test_accuracy_list.append(test_accuracy)
    print('train loss list........',train_loss_list)
    print('valiation loss list........',validation_loss_list)
    print('test loss list.........',test_loss_list)

print('training finished....')
from pathlib import Path
MODEL_PATH=Path("models")
MODEL_PATH.mkdir(parents=True,exist_ok=True)
MODEL_NAME="Dataset3_Google_Net.pth"
MODEL_SAVE_PATH=MODEL_PATH/MODEL_NAME
print(f"Model saving in...{MODEL_SAVE_PATH}")
torch.save(obj=model.state_dict(),f=MODEL_SAVE_PATH)

print(train_loss_list)
print(validation_loss_list)
print(test_loss_list)
print(epochs_list)

# numpy_training_loss_list=[tensor.detach().numpy().astype(np.float64) for tensor in train_loss_list]
# numpy_array_training_loss= np.array(numpy_training_loss_list[1:])
# numpy_validation_loss_list=[tensor.detach().numpy().astype(np.float64) for tensor in validation_loss_list]
# numpy_array_validation_loss= np.array(numpy_validation_loss_list[1:])
# numpy_test_loss_list=[tensor.detach().numpy().astype(np.float64) for tensor in test_loss_list]
# numpy_array_test_loss= np.array(numpy_test_loss_list[1:])
# print(numpy_array_training_loss.shape,numpy_array_validation_loss.shape,numpy_array_test_loss.shape)

epochs_list

import matplotlib.pyplot as plt
from google.colab import files
fig,ax=plt.subplots()
ax.plot(epochs_list,train_loss_list,'g',label='Training Loss')
ax.plot(epochs_list,validation_loss_list,'b',label='Validation Loss')
ax.plot(epochs_list,test_loss_list,'r',label='Test Loss')
ax.plot()
ax.set_title('GoogLeNet loss Comparisions on dataset3')
ax.set_xlabel('Number of epochs')
ax.set_ylabel('Loss')
ax.legend()
plt.savefig('Dataset3_GoogleNet_Loss.png')
files.download('Dataset3_GoogleNet_Loss.png')
plt.show()

print(len(training_accuracy_list),len(validation_accuracy_list),len(test_accuracy_list))

import matplotlib.pyplot as plt
from google.colab import files
fig,ax=plt.subplots()
ax.plot(epochs_list,training_accuracy_list,'g',label='Training Accuracy')
ax.plot(epochs_list,validation_accuracy_list,'b',label='Validation Accuracy')
ax.plot(epochs_list,test_accuracy_list,'r',label='Test Accuracy')
ax.plot()
ax.set_title('GoogLeNet accuracy Comparisions on dataset3')
ax.set_xlabel('Number of epochs')
ax.set_ylabel('Accuracy')
ax.legend()
plt.savefig('Dataset3_GoogleNet_Accuracy.png')
files.download('Dataset3_GoogleNet_Accuracy.png')
plt.show()

#Evaluate the model's output
model.eval()
with torch.no_grad():
    correct = 0
    total = 0
    for images, labels in validation_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    accuracy = correct / total
    print('Accuracy on validation set: {}%'.format(100 * accuracy))

test_dataset = CustomDataset(images=X_test, labels=y_test, transform=transform)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
predicted_list=[]
labels_list=[]
model.eval()
with torch.no_grad():
    correct = 0
    total = 0
    for images, labels in test_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        predicted_list.append(predicted)
        labels_list.append(labels)
    accuracy = correct / total
    print('Accuracy on test set: {}%'.format(100 * accuracy))